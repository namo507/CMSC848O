{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 1, CMSC848O Spring 2025\n",
        "\n",
        "### This is due on **February 25th, 2025**, submitted via Gradescope as a PDF (File>Print>Save as PDF). 100 points total.\n",
        "\n",
        "### <font color=\"red\">**IMPORTANT**: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right. Change the access permissions from \"Restricted\" to \"Anyone with the link\" and then click the \"Copy link\" button. Paste the result below. If you fail to do this, you will receive no credit for this homework!\n",
        "# ***LINK: *replace this text with your link!****</font>\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing simple Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says \"Write your answer here...\" with your actual answer.\n",
        "\n",
        "- This assignment is designed so that you can run all cells almost instantly. If it is taking longer than that, you have made a mistake in your code.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to submit this problem set:*\n",
        "- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Gradescope.\n",
        "  \n",
        "- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
        "\n",
        "- **Important:** on Gradescope, please make sure that you tag each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions. We may take off points for submissions that are not tagged.\n",
        "\n",
        "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxa48WP8dXh9"
      },
      "source": [
        "##Question 1.1 (5 points)\n",
        "You have a unigram language model with a vocabulary of just two words:\n",
        "{cat,dog}. The model always predicts \"cat\" with probability 0.7 and \"dog\" with probability 0.3, regardless of any context. You have a small test sequence that is exactly four words long: \"cat cat dog cat\".\n",
        "\n",
        "\n",
        "Compute the perplexity of the model on this test sequence.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyrvDudjbPUO"
      },
      "source": [
        "**Write your answer here!** If you want partial credit, please include any intermediate equation(s) formatted in LaTeX in your answer. You can add LaTeX to your answer by wrapping it in $ signs; see the above cell for examples. If you've never used LaTeX before, please check out [this notebook](https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/00/intro_to_latex.ipynb) to get the hang of it!\n",
        "\n",
        "\n",
        "##Answer 1.1\n",
        "\n",
        "Alright so from the class lecture and notes, perplexity is a standard metric that tells us how well a model predicts a given text. Now, a lower perplexity means better prediction. Basically in simple terms, how surprised the model is by seeing the text. So more predictable the text, lower will be the perplexity. \n",
        "\n",
        "So in class notes, $Perplexity = exp(-\\frac{1}{n} \\sum_{i=1}^n log P_{lm}(t_i|t_{1...i-1}))$\n",
        "\n",
        "Now the question talks about unigram model, so words will be treated independently here. Now there are 4 words so n = 4. \n",
        "Given, P(cat) = 0.7 and P(dog) = 0.3 regardless of the context.\n",
        "\n",
        "Now, for this unigram model, $P_{lm}(t_i|t_{1...i-1}) = P(t_i)$\n",
        "Upon substituting the values based on the sequence \"cat cat dog cat\" :\n",
        "\n",
        "$Perplexity = exp(-\\frac{1}{4}[log(0.7) + log(0.7) + log(0.3) + log(0.7)])$\n",
        "\n",
        "Since $log(0.7) ≈ -0.357$ and $log(0.3) ≈ -1.204$\n",
        "\n",
        "$Perplexity = exp(-\\frac{1}{4}[-0.357 - 0.357 - 1.204 - 0.357])$ $= exp(-\\frac{1}{4}[-2.275])$ $= exp(0.569)$ $= 1.778$\n",
        "\n",
        "Hence this model has a perplexity of 1.778, meaning it's fairly confident about predicting the sequence \"cat cat dog cat.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsq0NIDzcfeC"
      },
      "source": [
        "##Question 1.2 (5 points)\n",
        "Let's keep going with the same language model from the previous question. Now, you receive a new test sequence: \"cat dog fish cat\". What is the perplexity of the model on this new test sequence?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLR5ML-dQ2t"
      },
      "source": [
        "**Write your answer here!** As before, if you want partial credit, please include intermediate computations in LaTeX here.\n",
        "\n",
        "##Answer 1.2\n",
        "\n",
        "Interesting. So in the previous question, it was mentioned that I have a unigram language model with a vocabulary of just two words i.e. cat and dog. Hence, the word fish is kinda out of the vocabulary. Hence P(fish) = 0\n",
        "Rest remains the same i.e. P(cat) = 0.7 and P(dog) = 0.3\n",
        "\n",
        "Now the perplexity formula was\n",
        "$Perplexity = exp(-\\frac{1}{n} \\sum_{i=1}^n log P_{lm}(t_i|t_{1...i-1}))$\n",
        "\n",
        "Now considering log of zero is undefined for the \"fish\" i.e. log of zero (log(0) -> undefined)probability, means that the perplexity overall will be undefined or infinite.\n",
        "\n",
        "Hence Perplexity = Infinite\n",
        "This actually indicates that the model is not capable of handling sequences with out-of-vocabulary words and so, leads to a complete uncertainty in its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2aVbUcYPXv"
      },
      "source": [
        "##Question 1.3 (5 points)\n",
        "Here is a simple way to build a language model: for any prefix $w_1, w_2, \\dots, w_{i-1}$, retrieve all occurrences of that prefix in some huge text corpus (such as the [Common Crawl](https://commoncrawl.org/)) and keep count of the word $w_i$ that follows each occurrence. Now, use these counts to estimate the conditional probability $P(w_i|w_1, w_2, \\dots, w_{i-1})$ for any prefix. Explain why this method is completely impractical!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crODlcAKfGcu"
      },
      "source": [
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "\n",
        "##Answer 1.3\n",
        "\n",
        "Ok so this method is impractical because if I apply permutation and combination of the prefixes, this will require enormous memory and computational resources to store and retrieve counts for all possible sequences. \n",
        "\n",
        "Not only that, but I remember the discussion about data sparsity which makes it impossible to compute reliable probabilities for many prefixes, and the approach fails to generalize to unseen contexts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1_cCBOwfPxI"
      },
      "source": [
        "##Question 2.1 (5 points)\n",
        "Let's switch over to some simple coding! The below coding cell contains the opening paragraph of Daphne du Maurier's novel *Rebecca*. Write some code in this cell to compute the number of unique word **types** and total word **tokens** in this paragraph. Use a whitespace tokenizer to separate words (i.e., split the string on white space using Python's split function, don't worry about handling punctuation properly). <font color=\"red\">Be sure that the cell's output (i.e., after running it) is visible in the PDF file you turn in on Gradescope.</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W9Fm6AQJQDFa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of word types: 76, number of word tokens:100\n"
          ]
        }
      ],
      "source": [
        "paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n",
        "that I was passing through the iron gates that led to the driveway.\n",
        "The drive was just a narrow track now, its stony surface covered\n",
        "with grass and weeds. Sometimes, when I thought I had lost it, it\n",
        "would appear again, beneath a fallen tree or beyond a muddy pool\n",
        "formed by the winter rains. The trees had thrown out new\n",
        "low branches which stretched across my way. I came to the house\n",
        "suddenly, and stood there with my heart beating fast and tears\n",
        "filling my eyes.'''.lower() # lowercase normalization is often useful in NLP\n",
        "\n",
        "types = 0\n",
        "tokens = 0\n",
        "\n",
        "# YOUR CODE HERE! POPULATE THE types AND tokens VARIABLES WITH THE CORRECT VALUES!\n",
        "words = paragraph.split() # Since the question talks about tokenizing the paragraph by splitting on white space\n",
        "types = len(set(words)) # types will get the unique words using set\n",
        "tokens = len(words) # tokens will get the total number of words\n",
        "\n",
        "# DO NOT MODIFY THE BELOW LINE!\n",
        "print('Number of word types: %d, number of word tokens:%d' % (types, tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5f5YpclYjbh"
      },
      "source": [
        "##Question 2.2 (5 points)\n",
        "Now let's look at the most frequently used word **types** in this paragraph. Write some code in the below cell to print out the ten most frequently-occurring types. We have initialized a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) object that you should use for this purpose. In general, Counters are very useful for text processing in Python. <font color=\"red\">Be sure that the cell's output (i.e., after running it) is visible in the PDF file you turn in on Gradescope.</font>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpjx2fGbh_tp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i 6\n",
            "the 6\n",
            "to 4\n",
            "a 3\n",
            "and 3\n",
            "my 3\n",
            "it 2\n",
            "that 2\n",
            "was 2\n",
            "with 2\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "# Okay so first intialize the counter with words from the paragraph used previously\n",
        "paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n",
        "that I was passing through the iron gates that led to the driveway.\n",
        "The drive was just a narrow track now, its stony surface covered\n",
        "with grass and weeds. Sometimes, when I thought I had lost it, it\n",
        "would appear again, beneath a fallen tree or beyond a muddy pool\n",
        "formed by the winter rains. The trees had thrown out new\n",
        "low branches which stretched across my way. I came to the house\n",
        "suddenly, and stood there with my heart beating fast and tears\n",
        "filling my eyes.'''.lower() # lowercase normalization is often useful in NLP\n",
        "\n",
        "c = Counter(words) # since words = paragraph.split() was defined previously\n",
        "\n",
        "# DO NOT MODIFY THE BELOW LINES!\n",
        "for word, count in c.most_common()[:10]:\n",
        "    print(word, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQfA98xLjIv9"
      },
      "source": [
        "##Question 2.3 (5 points)\n",
        "What do you notice about these words and their linguistic functions (i.e., parts-of-speech)? These words are known as \"stopwords\" in NLP and are often removed from the text before any computational modeling is done. Why do you think that is?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jj042SWkFvP"
      },
      "source": [
        "\n",
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "\n",
        "So looking at the most frequent words from the paragraph, I noticed that they are primarily function words/ determiners like \"the\", \"and\", \"to\", \"I\" which are grammatical elements rather than say content carrying words. Now, these stopwords are typically removed in NLP because they don't contribute much semantic meaning to the text and can hinder the important content words that actually tells the user what the text is about. \n",
        "Not only that but removing them helps reduce noise and focus computational modeling on the more informative and content rich words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9OxWy-CYlp4"
      },
      "source": [
        "##Question 3.1 (10 points)\n",
        "In *neural* language models, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\\boldsymbol{x}$ of a given prefix, and then predict the probability of the next word conditioned on $\\boldsymbol{x}$. In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix \"Alice talked to\"; your job is to combine them into a single vector representation $\\boldsymbol{x}$ using [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). <font color=\"red\">Be sure that the cell's output (i.e., after running it) is visible in the PDF file you turn in on Gradescope.</font>\n",
        "\n",
        "*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Su_j1JY1QG5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prefix embedding tensor size:  torch.Size([3, 10])\n",
            "embedding sum:  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
            "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prefix = 'Alice talked to'\n",
        "\n",
        "# spend some time understanding this code / reading relevant documentation!\n",
        "# this is a toy problem with a 5 word vocabulary and 10-d embeddings\n",
        "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
        "vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}\n",
        "\n",
        "# we need to encode our prefix as integer indices (not words) that index\n",
        "# into the embeddings matrix. the below line accomplishes this.\n",
        "# note that PyTorch inputs are always Tensor objects, so we need\n",
        "# to create a LongTensor out of our list of indices first.\n",
        "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
        "prefix_embs = embeddings(indices)\n",
        "print('prefix embedding tensor size: ', prefix_embs.size())\n",
        "\n",
        "# okay! we now have three embeddings corresponding to each of the three\n",
        "# words in the prefix. write some code that adds them element-wise to obtain\n",
        "# a representation of the prefix! store your answer in a variable named \"x\".\n",
        "\n",
        "### YOUR CODE HERE!\n",
        "x = torch.sum(prefix_embs, dim=0) ##The question is to find the sum embeddings element-wise along dimension 0\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print('embedding sum: ', x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##Answer 3.1\n",
        "\n",
        "Ok so I will try explaining the output. The torch.Size([3, 10]) indicates it started with 3 words, each represented by a 10-dimensional vector. After element-wise addition, I will get a single 10-dimensional vector (second line of the code) that represents the entire prefix \"Alice talked to\". Hence, each number in this resulting vector captures some semantic information from the combination of all three words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41LYDeoZPYI"
      },
      "source": [
        "##Question 3.2 (5 points)\n",
        "Modern language models do not use element-wise addition to combine the different word embeddings in the prefix into a single representation (a process called *composition*). What is a major issue with element-wise functions that makes them unsuitable for use as composition functions?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MaDZekXs0C7"
      },
      "source": [
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "\n",
        "The main problem I see with element-wise addition is that it's commutative, meaning \"Alice talked to Bob\" and \"Bob talked to Alice\" would result in the exact same vector representation since the order doesn't matter when adding vectors. Basically, it fails to capture the interactions between words and their contextual nuances. So by simply adding the vectors, one loses the order and relationships between the words and hence resulting in a flat and less meaningful representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xGz2_cZVs7"
      },
      "source": [
        "##Question 3.3 (10 points)\n",
        "One very important function in neural language models (and for basically every task we'll look at this semester) is the [softmax](https://pytorch.org/docs/main/generated/torch.nn.functional.softmax.html#torch.nn.functional.softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Let's say we have our prefix representation $\\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \\text{softmax}(W\\boldsymbol{x})$. Let's explore this in the code cell below. <font color=\"red\">Be sure that the cell's output (i.e., after running it) is visible in the PDF file you turn in on Gradescope.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mClmHIeowL4V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "probability distribution tensor([0.5722, 0.3568, 0.0414, 0.0206, 0.0089], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "import torch.nn.functional as F  # Importing the functional module to apply softmax\n",
        "\n",
        "# remember, our goal is to produce a probability distribution over the\n",
        "# next word, conditioned on the prefix representation x. This distribution\n",
        "# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).\n",
        "# take a look at the dimensionality of x, and you'll notice that it is a\n",
        "# 10-dimensional vector. first, we need to **project** this representation\n",
        "# down to 5-d. We'll do this using the below matrix:\n",
        "\n",
        "W = torch.rand(10, 5)\n",
        "\n",
        "# use this matrix to project x to a 5-d space, and then\n",
        "# use the softmax function to convert it to a probability distribution.\n",
        "# this will involve using PyTorch to compute a matrix/vector product.\n",
        "# look through the documentation if you're confused (torch.nn.functional.softmax)\n",
        "# please store your final probability distribution in the \"probs\" variable.\n",
        "\n",
        "### YOUR CODE HERE\n",
        "projected = torch.matmul(x, W)  # Matrix multiplication using W and x from the previous question\n",
        "probs = F.softmax(projected, dim=0)  # Appling softmax function to the projected matrix\n",
        "\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE!\n",
        "print('probability distribution', probs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOlrqnSqZ3H8"
      },
      "source": [
        "##Question 3.4 (15 points)\n",
        "So far, we have looked at just a single prefix (\"Alice talked to\"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations. This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones). <font color=\"red\">Be sure that the cell's output (i.e., after running it) is visible in the PDF file you turn in on Gradescope.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OZarWwkESM7-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch embedding tensor size:  torch.Size([2, 3, 10])\n",
            "batch probability distributions: tensor([[0.5722, 0.3568, 0.0414, 0.0206, 0.0089],\n",
            "        [0.5722, 0.3568, 0.0414, 0.0206, 0.0089]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# for this problem, we'll just copy our old prefix over two times\n",
        "# to form a batch. in practice, each example in the batch would be different.\n",
        "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
        "batch_embs = embeddings(batch_indices)\n",
        "print('batch embedding tensor size: ', batch_embs.size())\n",
        "\n",
        "# now, follow the same procedure as before:\n",
        "# step 1: compose each example's embeddings into a single representation\n",
        "\n",
        "batch_x = torch.sum(batch_embs, dim=1)  # This results in a tensor of shape [2, 10]\n",
        "\n",
        "# using element-wise addition. HINT: check out the \"dim\" argument of the torch.sum function!\n",
        "\n",
        "# step 2: project each composed representation into a 5-d space using matrix W\n",
        "batch_projected = torch.matmul(batch_x, W)  # This results in a tensor of shape [2, 5]\n",
        "\n",
        "# step 3: use the softmax function to obtain a 2x5 matrix with the probability distributions\n",
        "\n",
        "# please store this probability matrix in the \"batch_probs\" variable, which is\n",
        "# currently initialized with random numbers.\n",
        "\n",
        "batch_probs = F.softmax(batch_projected, dim=1)  # This finally results in shape [2, 5]\n",
        "\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print(\"batch probability distributions:\", batch_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTxHxdG5BpQU"
      },
      "source": [
        "## Question 4 (30 points)\n",
        "\n",
        "Find  one academic paper about long-context language models (use e.g., [Semantic Scholar](https://www.semanticscholar.org/search?q=long%20context%20language%20models&sort=relevance) to search for relevant papers) that is of interest to you. Then, write a summary in your own words of the paper you chose. Your summary should answer the following questions: what is its motivation? Why should anyone care about it? How does it work? Were there things in the paper that you didn't understand at all? What were they? Fill out the below cell, and make sure to write 2-4 paragraphs for the summary to receive full credit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpg8eU9wNBci"
      },
      "source": [
        "**Title of paper**: Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell\n",
        "\n",
        "**Authors**: Taiming Lu, Muhan Gao, Kuai Yu, Adam Byerly, Daniel Khashabi\n",
        "\n",
        "**URL**: https://www.semanticscholar.org/paper/Insights-into-LLM-Long-Context-Failures%3A-When-Know-Lu-Gao/2138841e6e5ecf59d881a0108e0d9551484cfe32\n",
        "\n",
        "**Your summary**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1JWf6Qm9iQ"
      },
      "source": [
        "## AI Disclosure\n",
        "\n",
        "*   Did you use any AI assistance to complete this homework? If so, please also specify what AI you used.\n",
        "    * *your response here*\n",
        "\n",
        "    Yes. Mixture of Claude sonnet and Bing AI to refine the codes along with correct pdf latex format for the formulae used in the class lecture notes.\n",
        "\n",
        "\n",
        "---\n",
        "*(only complete the below questions if you answered yes above)*\n",
        "\n",
        "*   If you used a large language model to assist you, please paste *all* of the prompts that you used below. Add a separate bullet for each prompt, and specify which problem is associated with which prompt.\n",
        "    * *your response here*\n",
        "*   **Free response**: Describe your overall experience with the AI. How helpful was it? Did it just directly give you a good answer, or did you have to edit it? Was its output ever obviously wrong or irrelevant? Did you use it to get the answer or check your own answer?\n",
        "    * *your response here*\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
